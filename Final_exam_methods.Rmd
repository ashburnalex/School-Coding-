---

output:
  pdf_document: default
  word_document: default
  html_document: default
---
```{r}

knitr::opts_chunk$set(error = TRUE)
getwd()
setwd("C:\\Users\\uttva\\OneDrive\\Desktop\\Rstuff")
library(mgcv)
library(stats)
library(stats4)

```


```{r}
#1a
x1<-52.1
x2<-26.4
s1<-45.1
s2<-26.4
#zcore for second group to get into 95% of the first group
# The standard deviation of the first group , sd=45.1, is so large that it overlaps with most of the results of the second group
x1-(1.96*s1)
# the 95% of the first group overlaps all possible values of the second group, mean=26.4, therefore there is no evidence that there is a difference betweeen the group 
x2+1.96*s2
test<-(78.144-x1)/s1
# if there is a difference there is a 28% chance that there is one 
1-pnorm(test)
```


```{r}
#1b
qnorm(.95)



#standard deviation=
sde<-.035/qnorm(.95)
sde
0+(qnorm(.95)*sde)
qnorm(.99)

c(sde*-(qnorm(.99)),sde*qnorm(.99))
```


```{r}
#2
prob_2<-read.csv("final_problem2_data.csv")
head(prob_2)
help(boxplot)
# in general as the number of wheelss goes up the price shifts up, except from 3:4, some overlap between 0:3
boxplot(prob_2$total_pr~prob_2$wheels,ylim=c(1,200))

# price~cond
# new condition games are worth more generally than used games, but some used games scored higher(outliers), possibly the number of wheels based on the first graph
boxplot(prob_2$total_pr~prob_2$cond,ylim=c(1,130))
```


```{r}
#2b
prob_2$cond
help(lm)

prob_2$condnew<-0
prob_2$condused<-0
unique(prob_2$cond)
help(ifelse)
prob_2$cond[i]
prob_2$cond[2]=='new'
nrow(prob_2)

for (i in 1:nrow(prob_2)){
  if(prob_2$cond[i]=='new'){
  prob_2$condnew[i]<- 1}
  else {
  prob_2$condused[i]<-1 
}
  }

length(prob_2)
prob_2[13:14]

# regression

test<-summary(lm(prob_2$total_pr~0+prob_2$condnew+prob_2$stock_photo+prob_2$duration+prob_2$wheels))


test
```


```{r}
#2c
help("cooks.distance")
lm.influence(lm(prob_2$total_pr~prob_2$condnew+prob_2$stock_photo+prob_2$duration+prob_2$wheels-1))

cooksd<-cooks.distance(lm(prob_2$total_pr~prob_2$condnew+prob_2$stock_photo+prob_2$duration+prob_2$wheels-1))
help(sort)
attributes(cooksd)
sort(cooksd,decreasing=T)[1:5]
# Cooks method 
test_cooks<-summary(lm(total_pr~0+condnew+stock_photo+duration+wheels,,
           data=prob_2[-c(20,65,84,73,99),]))
summary(lm(total_pr~0+condnew+stock_photo+duration+wheels,
           data=prob_2[-c(20,65,84,73,99),]))$"adj.r.squared"

hist(prob_2$total_pr)
min(sort(prob_2$total_pr,decreasing = T)[1:2])
prob_2[prob_2$total_pr %in%sort(prob_2$total_pr,decreasing = T)[1:2],]

test_price_outliers<-summary(lm(total_pr~0+condnew+stock_photo+duration+wheels,
           data=prob_2[prob_2$total_pr<min(sort(prob_2$total_pr,decreasing = T)[1:2]),]))
```


```{r}
# 2c outliers

test_cooks$adj.r.squared
test_price_outliers$adj.r.squared
test_cooks$adj.r.squared>test_price_outliers$adj.r.squared
# cooks was the best way to get rid of outliers
#cooks prices
prob_2$total_pr[c(20,65,84,73,99)]
# outlier price
sort(prob_2$total_pr,decreasing = T)[1:2]
# outlier method captures the first two values
prob_2$total_pr[c(20,65,84,73,99)]==sort(prob_2$total_pr,decreasing = T)[1:2]
# parsimonious model
test_cooks
```


```{r}
# 2c continueed
# coeficents that are signifigant
# these are the variables i would include 
  names(test_cooks$coefficients[,4][test_cooks$coefficients[,4]<.05])
summary(lm(formula = total_pr ~ 0 + condnew + stock_photo  + 
    wheels, data = prob_2[-c(20, 65, 84, 73, 99), ]))
test_no_duration<-summary(lm(formula = total_pr ~ 0 + condnew + stock_photo  + 
    wheels, data = prob_2[-c(20, 65, 84, 73, 99), ]))
# better parsimonious model
test_no_duration$adj.r.squared>test_cooks$adj.r.squared
```


```{r}
#2d
str(prob_2)
prob_2$wheels
# using wheels as numeric the price goes up by 7.3 dollars per wheel
summary(lm(formula = total_pr ~ 0 + condnew + stock_photo  + 
    wheels, data = prob_2[-c(20, 65, 84, 73, 99), ]))
# using factor you get a value for every increased wheel since it is not perfectly linear, this is prefered because I get to see the effect of each increased wheel because they are greater than the sum of its parts 
summary(lm(formula = total_pr ~ 0 + condnew + stock_photo  + 
    as.factor(wheels), data = prob_2[-c(20, 65, 84, 73, 99), ]))
```


```{r}
#2e
# when using it as the only variable 
# for every bid expect the price to be three dollars more
summary(lm(total_pr~0+n_bids,data=prob_2[-c(20, 65, 84, 73, 99), ]))
```


```{r}
# not signifigant when compared to other statistically signifigant variables, for every bid expect to sell your game for 11 cents less, preffered method. Possibly a buy it now function? Or possibly the auctions that have a higher price have less bids because people do not want to spend money, ie not as attrative of an auction
summary(lm(formula = total_pr ~ 0 + condnew + stock_photo  + 
    as.factor(wheels)+n_bids, data = prob_2[-c(20, 65, 84, 73, 99), ]))
```


```{r}
#3
weather<-read.csv("case07.csv")
head(weather)
summary(weather)

# v1=year,v2=month,v3=day,v4=max,v5=min
# subsetting
# winter months 12,1,2
str(weather)
weather$flucuation<-weather$V4-weather$V5
winter<-subset(weather,weather$V2 %in% c(12,1,2) & weather$V4 !=-998 &weather$V5 !=-998)

summer<-subset(weather,weather$V2 %in% c(6,7,8) & weather$V4 !=-998 &weather$V5 !=-998)


# summer months 6,7,8
# ttest
t.test(summer$flucuation==winter$flucuation)
```


```{r}
#periods
old<-subset(weather,weather$V1<=1949 & weather$V1>=1900 & weather$V4 !=-998 &weather$V5 !=-998)
new<-subset(weather,weather$V1<=2000 & weather$V1>=1950 & weather$V4 !=-998 &weather$V5 !=-998)
# they are different
t.test(old$flucuation==new$flucuation)
```


```{r}
#autocorrelation
for (i in 2:nrow(weather)){
weather$flucuation_lag[i]<-weather$flucuation[i-1]
}
head(weather$flucuation_lag)
summary(lm(flucuation~0+flucuation_lag,data=weather[weather$V4 !=-998 &weather$V5 !=-998,]))
```


```{r}
# year
help(aggregate)
c(unique(weather$V1))
head(as.list(unique(weather$V1)))
# the advantage of this method is that there is more independence between years than in months, a hot month is ore likely to be followed by another hot month
early_year<-aggregate(flucuation~V1,FUN=mean,data=weather[weather$V1<=1949 & weather$V1>=1900 & weather$V4 !=-998 &weather$V5 !=-998,])

late_year<-early_year<-aggregate(flucuation~V1,FUN=mean,data=weather[weather$V1<=2000 & weather$V1>=1950 & weather$V4 !=-998 &weather$V5 !=-998,])
head(late_year$flucuation)
# there is no difference in this method
t.test(x=early_year$flucuation,y=late_year$flucuation,paired = F)
help(t.test)
```


```{r}
# time 
time<-aggregate(flucuation~V1,FUN=mean,data=weather[  weather$V4 !=-998 &weather$V5 !=-998,])
time$lag<-0
for ( i in 2:nrow(time)){
  time$lag[i]<-time$flucuation[i-1]
}
time$lag[c(2:nrow(time))]
head(time)
# advantags are the higher r2 score and less interdependence in theory, however if they werent interdependent would i be able to use it to predict # b and d were on t tests not linear models so  idk how to compare
summary(lm(lag~0+flucuation,data=time[c(2:nrow(time)),]))
```


```{r}
# 4 
news<-read.csv('datanews2019.csv')
head(news)

```


```{r}
# dates
help(substr)
help(as.Date)
head(as.character(news$V1))
#as.Date(as.character(news$V1),"%Y%M%D")
#help("as.Date.numeric")
#as.Date.default(news$V1)
#as.Date.POSIXct(news$V1)
#as.Date.numeric(news$V1,format="%Y-%m-%d",)
```


```{r}
# continuing without the day 
news$negative<-news$V3/news$V2


summary(lm(negative~0+X1,data=news))

# 1905
# could not figure out dates so just subset the data by year 
news$year<-as.numeric(substr(news$V1,1,4))
head(news$year)
news$old<-0
news$new<-0
nrow(news)


for ( i in 1:nrow(news)){
  if (news$year[i]>=1905 & news$year[i] <=1951){
    news$old[i]<-1
  }
  else {
    news$new[i]<-1
  }
}
# difference 
test<-lm(negative~0+X1+new+old,data=news)
summary(test)
test$coefficients
library(mgcv)
library(car)
# there is a difference 
linearHypothesis(test,"new=old")
```


```{r}
names(news)
news$x2_5<-sum(news[1,7:10])
sum(news[1,7:10])
head(news[7:10])
news[1,7:10]
 for (i in 1:nrow(news)){
   news$x2_5[i]<-sum(news[i,7:10])/4
   news$x6_10[i]<-sum(news[i,11:14])/4
   

 }

test<-lm(negative~0+X1+x2_5+x6_10,data=news)
summary(test)
```


```{r}
# naming error
#linearHypothesis(test,"negative=x2_5")

```


